## Overview
This exercise consists of the following parts:
Use given data in order to construct a graph using an adjacency list representation.使用给定的数据，使用邻接表表示构建图。  
Implement Prim’s algorithm for finding minimum spanning trees.实现Prim的算法来寻找最小生成树。  
Use the result of Prim’s algorithm to solve a clustering problem by deleting edges from the MST, then checking which nodes are connected.使用Prim算法的结果通过从MST中删除边来解决聚类问题，然后检查哪些节点是连接的。

## Motivating Application: Unsupervised Learning
There are two primary categories of machine learning tactics. Supervised learning involves using labeled data to train a model that can later apply labels to new data. For example, suppose I had a bunch of photos of Labradors (a dog breed) that I wanted to separate into black labs, yellow labs, and chocolate labs (different varieties of Labradors different only by coat color). If I manually pre-labelled these photos with the correct variety then I could use them to train a supervised learning algorithm to label future photos with the correct variety of Labrador.机器学习策略主要有两类。监督学习涉及使用标记数据来训练模型，该模型可以在以后将标签应用于新数据。例如，假设我有一堆拉布拉多犬（一种犬种）的照片，我想把它们分为黑色实验室、黄色实验室和巧克力实验室（不同品种的拉布拉多犬只在皮毛颜色上不同）。如果我手动用正确的品种预先标记这些照片，那么我就可以用它们来训练一个监督学习算法，用正确的拉布拉多品种标记未来的照片。

An unsupervised model, on the other hand, does not require any pre-labelling of the training data. Instead, unsupervised models attempt to discern patterns in the training data by looking only at the data itself. For example, we might be able to plot the photos of Labradors as points in some cartesian space (as a simple example, we might have a 3-dimensional point per picture which represents the average RGB values of the image). With these points, an unsupervised model might identify clusters of points that are similar to one another and conclude that points in the same cluster must be the same Labrador variety. The advantage to these "clustering" algorithms is that we do not need to take the effort to pre-label, one disadvantage is that it does not tell us which cluster represents which variety.另一方面，无监督模型不需要对训练数据进行任何预标记。相反，无监督模型试图通过只查看数据本身来辨别训练数据中的模式。例如，我们可能能够将拉布拉多犬的照片绘制为笛卡尔空间中的点（举个简单的例子，每张图片可能有一个三维点，代表图像的平均RGB值）。有了这些点，无监督模型可以识别出彼此相似的点簇，并得出结论，同一簇中的点必须是相同的拉布拉多品种。这些“聚类”算法的优点是我们不需要努力预先标记，一个缺点是它不能告诉我们哪个聚类代表哪个品种。

## Problem Statement
The most commonly used clustering algorithm is called k-means clustering. Given a collection of points and a number of clusters k, the k-means clustering algorithm will split those points into k collections such that points which share a cluster are those which are nearest to the same reference point. For this assignment we will be doing a different form of clustering that I will call k-margin clustering. For this algorithm, we will be given a collection of items and an integer k. We will then split the items into k collections in such a way to maximize the “gap” between the closest two clusters. We define the “gap” size between two clusters to be the closest pair of items between them. In other words, we want to split our collection into k subsets such that we have maximized the closest pair of points across those subsets.最常用的聚类算法称为k-means聚类。给定一组点和多个聚类k，k-means集群算法会将这些点拆分为k个集合，这样共享一个聚类的点就是最接近同一参考点的点。对于这个任务，我们将进行一种不同形式的聚类，我称之为k-argin聚类。对于这个算法，我们将得到一个项目集合和一个整数k。然后，我们将把项目拆分为k个集合，以最大化最接近的两个聚类之间的“差距”。我们将两个集群之间的“差距”大小定义为它们之间最接近的一对项目。换句话说，我们想把我们的集合分成k个子集，这样我们就可以最大化这些子集中最接近的一对点。

This task may seem daunting at first, but minimum spanning trees will help! To begin with, we will represent our collection of items as a 2-d array, such that cell i,j of this array represents the distance from item i to item j. Effectively, this 2-d array can be considered as an undirected, weighted, complete graph. So we now have a graph where all of the items are nodes and the weights of the edges between nodes represent their distance from each other. From here, we will calculate a minimum spanning tree of this graph. This will be helpful because:这项任务起初可能看起来很艰巨，但最小跨度的树会有所帮助！首先，我们将把我们的项目集合表示为一个二维数组，这样这个数组的单元格i，j表示从项目i到项目j的距离。实际上，这个二维数组可以被视为一个无向、加权、完整的图。因此，我们现在有一个图，其中所有项目都是节点，节点之间边的权重表示它们彼此之间的距离。从这里，我们将计算此图的最小生成树。这将是有益的，因为：

1. If we consider any cluster in the graph, the edge which connects that cluster to its closest neighboring cluster must be an edge in a minimum spanning tree of the graph. Suppose that the cost of the clustering is the weight of edge (x, y). This follows from the cut theorem of MSTs. We will define a cut in the graph such that our cluster is one side of the cut and all other nodes are on the other side. In this case, any edge going from this cluster to another cluster will cross the cut. The closest pair will then be the lightest edge which crosses the cut, and therefore is part of a minimum spanning tree!如果我们考虑图中的任何簇，则将该簇连接到其最近的相邻簇的边必须是图的最小生成树中的边。假设聚类的成本是边（x，y）的权重。这是根据MST的切割定理得出的。我们将在图中定义一个切割，使得我们的集群是切割的一侧，所有其他节点都在另一侧。在这种情况下，从这个簇到另一个簇的任何边都会穿过切割。最接近的一对将是穿过切割的最轻边，因此是最小生成树的一部分！
2. Being a spanning tree, a minimum spanning tree is connected and acyclic and contains n − 1 edges. If any edge is removed then the graph is no longer connected, instead it will have two separate components. If any two edges are removed then it will have three separate components. So if we remove k-1 edges then we will have k separate components.作为生成树，最小生成树是连通的、无环的，包含n-1条边。如果删除任何边，则图形将不再连接，而是有两个单独的组件。如果删除任何两条边，则它将有三个单独的组件。因此，如果我们去除k-1边，那么我们将有k个单独的分量。

Combining these two observations above, we get a clustering algorithm.结合上述两个观察结果，我们得到了一个聚类算法。
- Our input will be an n x n array of doubles, representing the pairwise distances of all our items (which we’ll just consider to be ints 0 through n-1, so the indices are the items) and an int k for the number of clusters我们的输入将是一个n x n的double数组，表示我们所有项目的成对距离（我们只考虑从整数0到n-1，所以索引就是项目），还有一个整数k表示簇的数量
- We will construct a graph using this n x n array.我们将使用这个n x n数组构造一个图。
- Next we’ll construct a minimum spanning tree on that graph接下来，我们将在该图上构建一个最小生成树
- Then we remove the k-1 heaviest edges from the MST(the weight of the last edge removed will be the distance between the closest pair of clusters)然后我们从MST中删除k-1最重的边（最后删除的边的权重将是最近的一对簇之间的距离）
- Finally we identify which items are in which cluster by checking which items are connected to each other using the remaining edges of the MST (e.g. by using a breadth-first search).最后，我们通过使用MST的剩余边（例如，通过使用广度优先搜索）检查哪些项目彼此连接，来确定哪些项目在哪个集群中。
Your task for this assignment is to implement that algorithm above.你的任务是实现上面的算法。

Input Format: For this assignment, input will be encoded in txt files. Supposing that each test consists of n items, the files will contain n+2 lines as follows:输入格式：对于此任务，输入将以txt文件编码。假设每个测试由n个项目组成，文件将包含n+2行，如下所示：
- The first line contains the value of k, i.e. the number of clusters to break the items into第一行包含k的值，即将项目分解为的簇的数量
- The next line contains the value n (the number of items)下一行包含值n（项目数）
- The remaining n lines contain a space-separated list of doubles which indicate the distances between each pair of items剩余的n行包含一个空格分隔的双精度列表，表示每对项目之间的距离

3
5
0 18 21 23 5
18 0 54 30 31
21 54 0 15 32
23 30 15 0 15
5 31 32 15 0


This file indicates that we will be splitting 5 items into k clusters. When doing so, item 0 is distance 18 from item 1, item 2 is distance 15 away from item 3, etc.此文件表示我们将把5个项目分成k个集群。这样做时，项目0与项目1的距离为18，项目2与项目3的距离为15，以此类推。

In this case the optimal clustering would be for the first cluster to contain items 0 and 4, the second to contain items 2 and 3, and the third cluster to contain item 1. To see the cost of this clustering we look at the closest pair of items for each pair of clusters, and then save the smallest. So in this case the cost of the clustering would be 15. The following explains why:在这种情况下，最佳聚类将是第一个聚类包含项目0和4，第二个聚类包含项2和3，第三个聚类包含项1。为了了解这种聚类的成本，我们查看每对聚类中最接近的一对项目，然后保存最小的一对。因此，在这种情况下，集群的成本将是15。以下解释了原因：
- The distance between the first cluster and the second is 15 because the closest pair of points that crosses these clusters is items 4 and 3 which have distance 15.第一簇和第二簇之间的距离为15，因为穿过这些簇的最接近的一对点是距离为15的项目4和3。
- The distance between the first cluster and the third is 18 because the closest pair of points that crosses these clusters is items 0 and 1 which have distance 18.第一簇和第三簇之间的距离为18，因为穿过这些簇的最接近的一对点是距离为18的项目0和1。
- The distance between the second cluster and the third is 30 because the closest pair of points that crosses these clusters is items 1 and 3 which have distance 30.第二簇和第三簇之间的距离为30，因为穿过这些簇的最接近的一对点是距离为30的项目1和3。
- The overall cost of the clustering is the smallest of these distances, and so it is 15.集群的总成本是这些距离中最小的，因此为15。

## Provided Code
Several java classes have been provided for you in this zip file. Here is a description of each.
`Client`
This class contains the main method, which does the following:
- Reads the input file named in the field testFileName
- Parses the file and converts all the information into a 2-d array along with an integer variable k.
- Calls the constructor that you will implement, providing the array and k as input
- Prints the cost of the clustering found.
读取字段testFileName中命名的输入文件  
解析文件并将所有信息转换为二维数组以及整数变量k。  
调用您将要实现的构造函数，提供数组和k作为输入  
打印找到的聚类成本。  
Do not submit this file

`WeightedEdge`
These objects will be used to represent an edge in the graph (if you choose to use an adjacency list representation, which I think is easier).这些对象将用于表示图中的边（如果你选择使用邻接表表示，我认为这更容易） Its primary role is just to encapsulate 3 fields:
- source: the source node of the edge 边的源节点
- destination: the destination node of the edge
- weight: the weight of the edge
source：边的源节点
destination：边的目标节点
weight：边缘的重量
You are not required to use this class, but I found it very helpful!
Do not submit this file

`Clusterer`
This is the file that you will be modifying for this assignment. Your goal is to implement the constructor, which must calculate the clusters for the given 2-d array of distances and integer k. To guide you through this:这是您将为此任务修改的文件。你的目标是实现构造函数，它必须计算给定的二维距离数组和整数k的簇
- In part 1 you will convert the 2-d array into an adjacency list representation (you’re welcome to use the array itself as an adjacency matrix if you prefer, but I find the adjacency list easier to work with)在第1部分中，您将把二维数组转换为邻接表表示（如果您愿意，欢迎将数组本身用作邻接矩阵，但我发现邻接表更容易使用）
- In part 2 you will implement Prim’s algorithm to compute a MST for this graph在第2部分中，您将实现Prim的算法来计算此图的MST
- In part 3 you will remove edges from the minimum spanning tree, then check which nodes are connected to each other to identify the clusters.在第3部分中，您将从最小生成树中删除边，然后检查哪些节点相互连接以识别集群。
You will submit this file to Gradescope


`Various text files`
specExample.txt
The file described above, The cost of the clustering should be 15.0
2clusters500points.txt
The cost of this clustering should be 1703083.0
3clusters10points.txt
The cost of this clustering should be 119.0
5clusters20points.txt
The cost of this clustering should be 620.0
5clusters100points.txt
The cost of this clustering should be 37821.0
10clusters10points.txt
The cost of this clustering should be 22.0
13clusters50points.txt
The cost of this clustering should be 4248.0
20clusters1000points.txt
The cost of this clustering should be 4249168.0
11clusters121points.txt
The cost of this clustering should be 39360.0
35clusters35points.txt
The cost of this clustering should be 57.0

## Part 1: Making the graph
For this part you will build a graph out of the contents of the test files. The main method in Client reads the text file, then provides its contents as arguments to your constructor. These arguments are:在本部分中，您将根据测试文件的内容构建一个图形。Client中的main方法读取文本文件，然后将其内容作为参数提供给构造函数。这些论点是：
- `distances`: a 2-d array of doubles with the property that cell i,j contains the distance between items i and j. You may assume that all distances are positive, that cell i,j matches cell j,i, and that every cell i,i is 0.一个二维双精度数组，属性为单元格i，j包含项目i和j之间的距离。你可以假设所有距离都是正的，单元格i，j与单元格j，i匹配，并且每个单元格i，i都是0。
- `k`: the number of clusters to break our data into 将我们的数据分成的簇的数量

There are several ways to implement the constructor, but the way I recommend is to build an adjacency list representation of the graph by making a list of lists of WeightedEdge objects. In this case we have a complete graph, meaning that an adjacency list does not have any space benefit over an adjacency matrix, nor will it have any time benefit. It’s also not going to be asymptotically worse by either metric, though. In my opinion, an adjacency list is easier to use because it allows for more readable code. If you prefer to use an adjacency matrix, though, I support your decision!有几种方法可以实现构造函数，但我建议的方法是通过创建WeightedEdge对象的列表来构建图的邻接列表表示。在这种情况下，我们有一个完整的图，这意味着邻接表与邻接矩阵相比没有任何空间优势，也没有任何时间优势。不过，无论从哪个指标来看，情况也不会逐渐恶化。在我看来，邻接列表更容易使用，因为它允许更易读的代码。不过，如果你更喜欢使用邻接矩阵，我支持你的决定！

## Part 2: Prim’s algorithm
For this part you will implement Prim’s algorithm for finding a minimum spanning tree. I recommend following the pseudocode presented in class/section. You’re welcome to use any list or priority queue data structures that you would like from java.util. I recommend that you use this algorithm to produce a second adjacency list to represent just the minimum spanning tree. That is, you will produce a new graph with the same nodes as the original, but there will be only the V-1 edges included in the minimum spanning tree. Keep in mind that this graph should be undirected.在这一部分中，您将实现Prim的算法来寻找最小生成树。我建议遵循类/节中提供的伪代码。欢迎您在java.util.中使用任何您想要的列表或优先级队列数据结构。我建议您使用此算法生成第二个邻接表，以仅表示最小生成树。也就是说，您将生成一个与原始图具有相同节点的新图，但最小生成树中只包含V-1边。请记住，此图应该是无向的。

## Part 3: Clustering
Now we will use the minimum spanning tree to produce the clusters. The first step is to identify the k-1 largest edges used in the minimum spanning tree. Then, remove them! When doing these steps, remember that every edge appears twice in an adjacency list representation of an undirected graph. Use the weight of the lowest-weight edge you removed as the value of the cost field of the object.现在我们将使用最小生成树来生成集群。第一步是识别最小生成树中使用的k-1最大边。然后，把它们拿走！执行这些步骤时，请记住，在无向图的邻接表表示中，每条边都会出现两次。使用您删除的最低权重边的权重作为对象的成本字段的值。

Once you’ve removed these k-1 edges you can identify which integers belong to which cluster. To do this, implement a breadth first search. You can start your breadth-first search from an arbitrary node, then all nodes visited from that one will belong to the same cluster. Then repeat a breadth first search on all nodes not yet visited by the previous searches. You can then assign the list of lists of connected nodes as the cluster field.一旦你删除了这些k-1边，你就可以确定哪些整数属于哪个簇。为此，实施广度优先搜索。您可以从任意节点开始广度优先搜索，然后从该节点访问的所有节点都将属于同一集群。然后对之前搜索尚未访问的所有节点重复广度优先搜索。然后，您可以将连接节点列表列表指定为集群字段。

This is the last thing to implement for this assignment! Once you’ve done this, verify correctness by trying out each of the txt files provided to make sure the cost is correct. Because there will be several clusterings of the same cost, the easiest way to verify your cluster assignment is to check every item with every item in all the other clusters, making sure that the distance never exceeds the cost. We don’t provide the code to do this, but the autograder will be checking this.这是本次作业中最后要执行的任务！完成此操作后，通过尝试提供的每个txt文件来验证正确性，以确保成本正确。由于将有多个成本相同的集群，因此验证集群分配的最简单方法是将每个项目与所有其他集群中的每个项目进行检查，确保距离永远不会超过成本。我们不提供执行此操作的代码，但签名者将对此进行检查。

